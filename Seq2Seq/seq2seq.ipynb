{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载数据\n",
    "import nltk\n",
    "import sys\n",
    "train_file = \"./data/train.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = sys.getdefaultencoding()\n",
    "def load_data(in_file):\n",
    "    cn = []\n",
    "    en = []\n",
    "    num_examples = 0\n",
    "    with open(in_file, 'r', encoding = enc) as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split(\"\\t\")\n",
    "            # comment the following line if you use nltk\n",
    "            en.append([\"BOS\"] + line[0].split() + [\"EOS\"])\n",
    "            # uncomment the following line if you installed nltk\n",
    "#             en.append([\"BOS\"] + nltk.word_tokenize(line[0]) + [\"EOS\"]) \n",
    "            # split chinese sentence into characters\n",
    "            cn.append([\"BOS\"] + [c for c in line[1]] + [\"EOS\"])\n",
    "    return en, cn\n",
    "train_en, train_cn = load_data(train_file)\n",
    "num_train = len(train_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['BOS', 'Anyone', 'can', 'do', 'that.', 'EOS'],\n",
       " ['BOS', 'How', 'about', 'another', 'piece', 'of', 'cake?', 'EOS'],\n",
       " ['BOS', 'She', 'married', 'him.', 'EOS'],\n",
       " ['BOS', 'I', \"don't\", 'like', 'learning', 'irregular', 'verbs.', 'EOS'],\n",
       " ['BOS', \"It's\", 'a', 'whole', 'new', 'ball', 'game', 'for', 'me.', 'EOS']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_en[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['BOS', '任', '何', '人', '都', '可', '以', '做', '到', '。', 'EOS'],\n",
       " ['BOS', '要', '不', '要', '再', '來', '一', '塊', '蛋', '糕', '？', 'EOS'],\n",
       " ['BOS', '她', '嫁', '给', '了', '他', '。', 'EOS'],\n",
       " ['BOS', '我', '不', '喜', '欢', '学', '习', '不', '规', '则', '动', '词', '。', 'EOS'],\n",
       " ['BOS',\n",
       "  '這',\n",
       "  '對',\n",
       "  '我',\n",
       "  '來',\n",
       "  '說',\n",
       "  '是',\n",
       "  '個',\n",
       "  '全',\n",
       "  '新',\n",
       "  '的',\n",
       "  '球',\n",
       "  '類',\n",
       "  '遊',\n",
       "  '戲',\n",
       "  '。',\n",
       "  'EOS']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cn[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建Vocabulary\n",
    "import os\n",
    "import collections\n",
    "import pickle\n",
    "\n",
    "def make_dir(path):\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError:\n",
    "        pass\n",
    "    \n",
    "model_dir = \"seq2seq\"\n",
    "make_dir(model_dir)\n",
    "\n",
    "def build_dict(sentences, max_words=50000):\n",
    "    word_count = collections.Counter()\n",
    "    for sentence in sentences:\n",
    "        for s in sentence:\n",
    "            word_count[s] += 1\n",
    "    # 取出现最多的前50000个字，按照出现次数进行排序\n",
    "    ls = word_count.most_common(max_words)\n",
    "    total_words = len(ls) + 1\n",
    "    # 索引和值调换个位置，并且加一个UNK表示Unknown文字\n",
    "    word_dict = {w[0]: index+1 for (index, w) in enumerate(ls)}\n",
    "    word_dict[\"UNK\"] = 0\n",
    "    return word_dict, total_words\n",
    "\n",
    "vocab_file = os.path.join(model_dir, \"vocab.pkl\")\n",
    "en_dict, en_total_words = build_dict(train_en)\n",
    "cn_dict, cn_total_words = build_dict(train_cn)\n",
    "    \n",
    "# Inverse字典，键值位置调换\n",
    "inv_en_dict = {v: k for k, v in en_dict.items()}\n",
    "inv_cn_dict = {v: k for k, v in cn_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['BOS', 'Anyone', 'can', 'do', 'that.', 'EOS'],\n",
       " ['BOS', 'How', 'about', 'another', 'piece', 'of', 'cake?', 'EOS'],\n",
       " ['BOS', 'She', 'married', 'him.', 'EOS'],\n",
       " ['BOS', 'I', \"don't\", 'like', 'learning', 'irregular', 'verbs.', 'EOS'],\n",
       " ['BOS', \"It's\", 'a', 'whole', 'new', 'ball', 'game', 'for', 'me.', 'EOS']]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 将word转换成index\n",
    "def encode(en_sentences, cn_sentences, en_dict, cn_dict, sort_by_len=True):\n",
    "    '''\n",
    "        Encode the sequences. \n",
    "    '''\n",
    "    length = len(en_sentences)\n",
    "    out_en_sentences = []\n",
    "    out_cn_sentences = []\n",
    "\n",
    "    for i in range(length):\n",
    "        # [1, 7808, 52, 32, 198, 2]\n",
    "        en_seq = [en_dict[w] if w in en_dict else 0 for w in en_sentences[i]]\n",
    "        cn_seq = [cn_dict[w] if w in cn_dict else 0 for w in cn_sentences[i]]\n",
    "        # [[1, 7808, 52, 32, 198, 2]]\n",
    "        out_en_sentences.append(en_seq)\n",
    "        out_cn_sentences.append(cn_seq)\n",
    "\n",
    "    # sort sentences by english lengths\n",
    "    def len_argsort(seq):\n",
    "        return sorted(range(len(seq)), key=lambda x: len(seq[x]))\n",
    "    \n",
    "    # 按照英文的长短进行排序\n",
    "    if sort_by_len:\n",
    "        sorted_index = len_argsort(out_en_sentences)\n",
    "        out_en_sentences = [out_en_sentences[i] for i in sorted_index]\n",
    "        out_cn_sentences = [out_cn_sentences[i] for i in sorted_index]\n",
    "    return out_en_sentences, out_cn_sentences\n",
    "\n",
    "display(train_en[:5])\n",
    "train_en, train_cn = encode(train_en, train_cn, en_dict, cn_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 5214, 2], [1, 4380, 2], [1, 2674, 2], [1, 8514, 2], [1, 4380, 2]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Run.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(train_en[:5])\n",
    "inv_en_dict[5214]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 7, 86, 441, 5, 3, 2],\n",
       " [1, 118, 1374, 219, 2],\n",
       " [1, 1012, 2016, 7, 3, 2],\n",
       " [1, 238, 238, 219, 2],\n",
       " [1, 152, 189, 219, 2]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('你', '用', '跑', '的', '。')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(train_cn[:5])\n",
    "inv_cn_dict[7],inv_cn_dict[86],inv_cn_dict[441],inv_cn_dict[5],inv_cn_dict[3],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据转换成batch\n",
    "batch_size = 128\n",
    "import numpy as np\n",
    "\n",
    "# get minibatches of \n",
    "# 把一万四千数据分成若干组，每组128个数据\n",
    "def get_minibatches(n, minibatch_size, shuffle=False):\n",
    "    idx_list = np.arange(0, n, minibatch_size)\n",
    "    # display(idx_list)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx_list)\n",
    "    minibatches = []\n",
    "    for idx in idx_list:\n",
    "        minibatches.append(np.arange(idx, min(idx + minibatch_size, n)))\n",
    "    return minibatches\n",
    "\n",
    "# 准备数据，不足的列补零，并且创建mask矩阵，原有数据为1，补零数据为0\n",
    "def prepare_data(seqs):\n",
    "    lengths = [len(seq) for seq in seqs]\n",
    "    n_samples = len(seqs)\n",
    "    max_len = np.max(lengths)\n",
    "\n",
    "    x = np.zeros((n_samples, max_len)).astype('int32')\n",
    "    x_mask = np.zeros((n_samples, max_len)).astype('float32')\n",
    "    for idx, seq in enumerate(seqs):\n",
    "        x[idx, :lengths[idx]] = seq\n",
    "        x_mask[idx, :lengths[idx]] = 1.0\n",
    "    return x, x_mask\n",
    "\n",
    "def gen_examples(en_sentences, cn_sentences, batch_size):\n",
    "    minibatches = get_minibatches(len(en_sentences), batch_size)\n",
    "    # display(minibatches)\n",
    "    all_ex = []\n",
    "    for minibatch in minibatches:\n",
    "        mb_en_sentences = [en_sentences[t] for t in minibatch]\n",
    "        # display(mb_en_sentences)\n",
    "        mb_cn_sentences = [cn_sentences[t] for t in minibatch]\n",
    "        mb_x, mb_x_mask = prepare_data(mb_en_sentences)\n",
    "        # display(mb_x, mb_x_mask)\n",
    "        mb_y, mb_y_mask = prepare_data(mb_cn_sentences)\n",
    "        all_ex.append((mb_x, mb_x_mask, mb_y, mb_y_mask))\n",
    "        # display(all_ex)\n",
    "    return all_ex\n",
    "train_data = gen_examples(train_en, train_cn, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "class Encoder:\n",
    "    def __init__(self, embedding, hidden_size, num_layers = 1):\n",
    "        self.embedding = embedding\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.cell = rnn.GRUCell(self.hidden_size)\n",
    "        \n",
    "    def __call__(self, inputs, seq_length, state=None):\n",
    "        out = tf.nn.embedding_lookup(self.embedding, inputs)\n",
    "        for i in range(self.num_layers):\n",
    "            out, state = tf.nn.dynamic_rnn(self.cell, out, sequence_length=seq_length, initial_state=state, dtype=tf.float32)\n",
    "        return out, state\n",
    "\n",
    "class Decoder:\n",
    "    def __init__(self, embedding, hidden_size, num_layers=1, max_length=15):\n",
    "        self.embedding = embedding\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.cell = rnn.GRUCell(hidden_size)\n",
    "        self.linear = tf.Variable(tf.random_normal(shape=(self.hidden_size, cn_total_words))*0.1)\n",
    "        \n",
    "        \n",
    "    def __call__(self, inputs, state, encoder_state): # context vector\n",
    "        \n",
    "        out = tf.nn.embedding_lookup(self.embedding, inputs)\n",
    "        out = tf.tile(tf.expand_dims(encoder_state, 1), (1, tf.shape(out)[1], 1))\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "#             state = tf.concat([state, encoder_state], 1)\n",
    "            out, state = tf.nn.dynamic_rnn(self.cell, out, initial_state=state, dtype=tf.float32)\n",
    "    \n",
    "        out = tf.tensordot(out, self.linear, axes=[[2], [0]])\n",
    "        return out, state\n",
    "\n",
    "class Seq2Seq:\n",
    "    def __init__(self, hidden_size, num_layers, embed_words_en, embed_words_cn):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.max_length = 15\n",
    "        self.grad_clip = 5.0\n",
    "        \n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            with tf.name_scope(\"place_holder\"):\n",
    "                self.encoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int64, name=\"encoder_inputs\")\n",
    "                self.encoder_length = tf.placeholder(shape=(None, ), dtype=tf.int64, name=\"encoder_length\")\n",
    "                self.decoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int64, name=\"decoder_inputs\")\n",
    "                self.decoder_target = tf.placeholder(shape=(None, None), dtype=tf.int64, name=\"decoder_target\")\n",
    "                self.decoder_mask = tf.placeholder(shape=(None, None), dtype=tf.float32, name=\"decoder_mask\")\n",
    "\n",
    "            with tf.name_scope(\"embedding\"):\n",
    "                self.embedding_en = tf.get_variable(name=\"embedding_en\", dtype=tf.float32, shape=(en_total_words, hidden_size),\n",
    "                                                    initializer=tf.constant_initializer(embed_words_en))\n",
    "                self.embedding_cn = tf.get_variable(name=\"embedding_cn\", dtype=tf.float32, shape=(cn_total_words, hidden_size),\n",
    "                                                    initializer=tf.constant_initializer(embed_words_cn))\n",
    "            with tf.name_scope(\"encoder-decoder\"):\n",
    "                self.encoder = Encoder(self.embedding_en, self.hidden_size, self.num_layers)\n",
    "                self.decoder = Decoder(self.embedding_cn + self.hidden_size, self.hidden_size, self.num_layers)\n",
    "\n",
    "            with tf.variable_scope(\"seq2seq-train\"):\n",
    "                encoder_outputs, encoder_state = self.encoder(self.encoder_inputs, self.encoder_length)\n",
    "                tf.get_variable_scope().reuse_variables()\n",
    "                # 这里把encoder的state送给decoder的state，encoder的output则不需要用了……\n",
    "                decoder_state = encoder_state\n",
    "                word_indices = self.decoder_inputs\n",
    "\n",
    "                decoder_outputs, decoder_state = self.decoder(word_indices, decoder_state, encoder_state)\n",
    "\n",
    "                # decoder_outputs.append(decoder_out)\n",
    "                decoder_outputs = tf.concat(decoder_outputs, 1)\n",
    "\n",
    "            with tf.name_scope(\"cost\"):\n",
    "                loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=decoder_outputs, labels=self.decoder_target)\n",
    "\n",
    "                self.cost = tf.reduce_mean(loss * self.decoder_mask)\n",
    "\n",
    "                tvars = tf.trainable_variables()\n",
    "                grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars), self.grad_clip)\n",
    "                optimizer = tf.train.RMSPropOptimizer(learning_rate=0.01)\n",
    "                self.train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "            with tf.variable_scope(\"seq2seq-generate\"):\n",
    "                self.generate_outputs = []\n",
    "                decoder_state = encoder_state\n",
    "                word_indices = tf.expand_dims(self.decoder_inputs[:, 0], 1)\n",
    "                for i in range(self.max_length):\n",
    "                    decoder_out, decoder_state = self.decoder(word_indices, decoder_state, encoder_state)\n",
    "                    softmax_out = tf.nn.softmax(decoder_out[:, 0, :])\n",
    "                    word_indices = tf.expand_dims(tf.cast(tf.argmax(softmax_out, -1), dtype=tf.int64), 1)\n",
    "                    self.generate_outputs.append(word_indices)\n",
    "                self.generate_outputs = tf.concat(self.generate_outputs, 0)\n",
    "            \n",
    "            \n",
    "    def train(self, sess, encoder_inputs, encoder_length, decoder_inputs, decoder_target, decoder_mask):\n",
    "        _, cost = sess.run([self.train_op, self.cost], feed_dict={\n",
    "            self.encoder_inputs: encoder_inputs, \n",
    "            self.encoder_length: encoder_length,\n",
    "            self.decoder_inputs: decoder_inputs,\n",
    "            self.decoder_target: decoder_target,\n",
    "            self.decoder_mask: decoder_mask\n",
    "        })\n",
    "        return cost\n",
    "    \n",
    "    def generate(self, sess, encoder_inputs, encoder_length):\n",
    "        decoder_inputs = np.asarray([[en_dict[\"BOS\"]]*15], dtype=\"int64\")\n",
    "        if encoder_inputs.ndim == 1:\n",
    "            encoder_inputs = encoder_inputs.reshape((1, -1))\n",
    "            encoder_length = encoder_length.reshape((-1))\n",
    "        generate = sess.run([self.generate_outputs],\n",
    "                           feed_dict={self.encoder_inputs: encoder_inputs,\n",
    "                                      self.decoder_inputs: decoder_inputs,\n",
    "                                      self.encoder_length: encoder_length})[0]\n",
    "        return generate\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.038896150617862627\n",
      "training loss: 0.027942053787576877\n",
      "training loss: 0.026126280200380523\n",
      "training loss: 0.02499888086486304\n",
      "training loss: 0.024169257508578897\n",
      "training loss: 0.02342231726602129\n",
      "training loss: 0.02275253400367543\n",
      "training loss: 0.022218813433877157\n",
      "training loss: 0.021750806375896086\n",
      "training loss: 0.02134941043885258\n",
      "training loss: 0.020977562951802108\n",
      "training loss: 0.020659926768910375\n",
      "training loss: 0.0203385693416833\n",
      "training loss: 0.020073575572564614\n",
      "training loss: 0.01979738133071598\n",
      "training loss: 0.019565929461270313\n",
      "training loss: 0.019342890808146416\n",
      "training loss: 0.01912959994448197\n",
      "training loss: 0.01895555056749991\n",
      "training loss: 0.0187674670671224\n",
      "training loss: 0.018629708392141214\n",
      "training loss: 0.018449202379848643\n",
      "training loss: 0.018309324588104987\n",
      "training loss: 0.018164931739392928\n",
      "training loss: 0.018027273817711374\n",
      "training loss: 0.017919452070804354\n",
      "training loss: 0.017767461875362208\n",
      "training loss: 0.01766828853150716\n",
      "training loss: 0.017556803623315746\n",
      "training loss: 0.017448224303137205\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "# 隐层有50个神经元\n",
    "hidden_size = 50\n",
    "num_layers = 1\n",
    "# 不用预训练的embedding，用2个随机正态分布的embedding\n",
    "emb_en = np.random.uniform(low=-0.1, high=0.1, size=(en_total_words, hidden_size))\n",
    "emb_cn = np.random.uniform(low=-0.1, high=0.1, size=(cn_total_words, hidden_size))\n",
    "model = Seq2Seq(hidden_size, num_layers, emb_en, emb_cn)\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "epoch = 0\n",
    "n_epochs = 30\n",
    "# print(sess.run(model.decoder_state))\n",
    "while epoch < n_epochs:\n",
    "    epoch += 1\n",
    "    total_loss = 0 \n",
    "    total_num_ins = 0\n",
    "    for (encoder_inputs, encoder_length, mb_y, mb_y_mask) in train_data:\n",
    "        decoder_inputs = mb_y[:, :-1]\n",
    "        decoder_target = mb_y[:, 1:]\n",
    "#         print(encoder_length.sum(1).shape)\n",
    "        loss = model.train(sess, encoder_inputs, encoder_length.sum(1), decoder_inputs, decoder_target, mb_y_mask[:, :-1])\n",
    "        total_loss += loss\n",
    "        total_num_ins += mb_y.shape[0]\n",
    "    print(\"training loss: {}\".format(total_loss / total_num_ins))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BOS', 'Can', 'he', 'speak', 'English?', 'EOS']\n",
      "['他', '能', '能', '說', '說', '語', '語', '語', '吗', '？', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK']\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs = [inv_en_dict[c] for c in train_data[11][0][2]]\n",
    "print(encoder_inputs)\n",
    "encoder_inputs = [en_dict.get(e, 0) for e in encoder_inputs]\n",
    "encoder_inputs = np.asarray(encoder_inputs).reshape(1, -1)\n",
    "encoder_length = np.asarray([encoder_inputs.shape[1]]).reshape(-1)\n",
    "res = model.generate(sess, encoder_inputs, encoder_length).flatten()\n",
    "\n",
    "res = [inv_cn_dict[r] for r in res]\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
