{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import os\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_DATA/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_DATA/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_DATA/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_DATA/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_DATA', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvModel(object):    \n",
    "    def __init__(self, lr, batch_size, iter_num):\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.iter_num = iter_num\n",
    "        \n",
    "        self.X_flat = tf.placeholder(tf.float32, [None, 784])\n",
    "        self.X = tf.reshape(self.X_flat, [-1, 28, 28, 1]) # 本次要用卷积进行运算，所以使用2维矩阵。从这个角度讲，利用了更多的位置信息。\n",
    "        self.y = tf.placeholder(tf.float32, [None, 10])\n",
    "        self.dropRate = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "        # 32个神经元\n",
    "        # kernel_size: An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window.\n",
    "        # Can be a single integer to specify the same value for all spatial dimensions. 5x5的滑窗\n",
    "        conv1 = tf.layers.conv2d(self.X, 32, 5, padding='same', activation=tf.nn.relu,\n",
    "                                     kernel_initializer=tf.truncated_normal_initializer(stddev=0.1, seed=0),\n",
    "                                     bias_initializer=tf.constant_initializer(0.1))        \n",
    "        conv1 = tf.layers.max_pooling2d(conv1 , 2,2)        \n",
    "        conv2 = tf.layers.conv2d(conv1, 64, 5, padding='same', activation=tf.nn.relu,\n",
    "                                     kernel_initializer=tf.truncated_normal_initializer(stddev=0.1, seed=0),\n",
    "                                     bias_initializer=tf.constant_initializer(0.1))\n",
    "        pool1 = tf.layers.max_pooling2d(conv2, 2,2)               \n",
    "        flatten = tf.reshape(pool1 , [-1, 7*7*64])\n",
    "        dense1 = tf.layers.dense(flatten, 1024,  activation=tf.nn.relu, use_bias=True,\n",
    "                                 kernel_initializer=tf.truncated_normal_initializer(stddev=0.1, seed=0),\n",
    "                                 bias_initializer=tf.constant_initializer(0.1))\n",
    "        dense1_ = tf.nn.dropout(dense1, self.dropRate)\n",
    "        dense2 = tf.layers.dense(dense1_, 10, activation=tf.nn.relu, use_bias=True,\n",
    "                                 kernel_initializer=tf.truncated_normal_initializer(stddev=0.1, seed=0),\n",
    "                                 bias_initializer=tf.constant_initializer(0.1))\n",
    "\n",
    "        self.loss = tf.losses.softmax_cross_entropy(onehot_labels=self.y, logits=dense2)\n",
    "        self.train_step = tf.train.AdamOptimizer(1e-4).minimize(self.loss )\n",
    "        \n",
    "        # 用于模型训练\n",
    "        self.correct_prediction = tf.equal(tf.argmax(self.y, axis=1), tf.argmax(dense2, axis=1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\n",
    "        # 用于保存训练好的模型\n",
    "        self.saver = tf.train.Saver()\n",
    "    def train(self):\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())  # 先初始化所有变量。\n",
    "            for i in range(self.iter_num):\n",
    "                batch_x, batch_y = mnist.train.next_batch(self.batch_size)   # 读取一批数据\n",
    "                loss, _= sess.run([self.loss, self.train_step], \n",
    "                                  feed_dict={self.X_flat: batch_x, self.y: batch_y, self.dropRate: 0.5})   # 每调用一次sess.run，就像拧开水管一样，所有self.loss和self.train_step涉及到的运算都会被调用一次。\n",
    "                if i%1000 == 0:                        \n",
    "                    train_accuracy = sess.run(self.accuracy, feed_dict={self.X_flat: batch_x, self.y: batch_y, self.dropRate: 1.})  # 把训练集数据装填进去\n",
    "                    test_x, test_y = mnist.test.next_batch(self.batch_size)\n",
    "                    test_accuracy = sess.run(self.accuracy, feed_dict={self.X_flat: test_x, self.y: test_y, self.dropRate: 1.})   # 把测试集数据装填进去\n",
    "                    print ('iter\\t%i\\tloss\\t%f\\ttrain_accuracy\\t%f\\ttest_accuracy\\t%f' % (i,loss,train_accuracy,test_accuracy))\n",
    "            self.saver.save(sess, 'model/mnistCNNModel') # 保存模型\n",
    "\n",
    "    def test(self):\n",
    "        with tf.Session() as sess:\n",
    "            self.saver.restore(sess, 'model/mnistCNNModel')\n",
    "            Accuracy = []\n",
    "            for i in range(int(10000/self.batch_size)):\n",
    "                test_x, test_y = mnist.test.next_batch(self.batch_size)\n",
    "                test_accuracy = sess.run(self.accuracy, feed_dict={self.X_flat: test_x, self.y: test_y, self.dropRate: 1.})\n",
    "                Accuracy.append(test_accuracy)\n",
    "            print('==' * 15) \n",
    "            print( 'Test Accuracy: ', np.mean(np.array(Accuracy))   ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rollo/tensorflow/lib/python3.4/site-packages/tensorflow/python/ops/losses/losses_impl.py:731: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "iter\t0\tloss\t11.177574\ttrain_accuracy\t0.109375\ttest_accuracy\t0.125000\n",
      "iter\t1000\tloss\t0.169426\ttrain_accuracy\t0.968750\ttest_accuracy\t0.984375\n",
      "iter\t2000\tloss\t0.028347\ttrain_accuracy\t1.000000\ttest_accuracy\t1.000000\n",
      "iter\t3000\tloss\t0.044962\ttrain_accuracy\t0.984375\ttest_accuracy\t0.984375\n",
      "iter\t4000\tloss\t0.182806\ttrain_accuracy\t0.937500\ttest_accuracy\t0.953125\n",
      "iter\t5000\tloss\t0.028693\ttrain_accuracy\t0.984375\ttest_accuracy\t1.000000\n",
      "iter\t6000\tloss\t0.020976\ttrain_accuracy\t1.000000\ttest_accuracy\t0.968750\n",
      "iter\t7000\tloss\t0.001591\ttrain_accuracy\t1.000000\ttest_accuracy\t1.000000\n",
      "iter\t8000\tloss\t0.012811\ttrain_accuracy\t1.000000\ttest_accuracy\t0.984375\n",
      "iter\t9000\tloss\t0.002953\ttrain_accuracy\t1.000000\ttest_accuracy\t0.953125\n",
      "iter\t10000\tloss\t0.042865\ttrain_accuracy\t0.984375\ttest_accuracy\t1.000000\n",
      "iter\t11000\tloss\t0.009167\ttrain_accuracy\t1.000000\ttest_accuracy\t1.000000\n",
      "iter\t12000\tloss\t0.001114\ttrain_accuracy\t1.000000\ttest_accuracy\t1.000000\n",
      "iter\t13000\tloss\t0.002437\ttrain_accuracy\t1.000000\ttest_accuracy\t1.000000\n",
      "iter\t14000\tloss\t0.000413\ttrain_accuracy\t1.000000\ttest_accuracy\t1.000000\n",
      "iter\t15000\tloss\t0.004533\ttrain_accuracy\t1.000000\ttest_accuracy\t1.000000\n",
      "iter\t16000\tloss\t0.000081\ttrain_accuracy\t1.000000\ttest_accuracy\t0.984375\n",
      "iter\t17000\tloss\t0.001598\ttrain_accuracy\t1.000000\ttest_accuracy\t1.000000\n",
      "iter\t18000\tloss\t0.001787\ttrain_accuracy\t1.000000\ttest_accuracy\t1.000000\n",
      "iter\t19000\tloss\t0.002130\ttrain_accuracy\t1.000000\ttest_accuracy\t1.000000\n",
      "iter\t20000\tloss\t0.000623\ttrain_accuracy\t1.000000\ttest_accuracy\t0.984375\n",
      "iter\t21000\tloss\t0.006578\ttrain_accuracy\t1.000000\ttest_accuracy\t1.000000\n",
      "iter\t22000\tloss\t0.123260\ttrain_accuracy\t1.000000\ttest_accuracy\t1.000000\n",
      "iter\t23000\tloss\t0.001634\ttrain_accuracy\t1.000000\ttest_accuracy\t0.984375\n",
      "iter\t24000\tloss\t0.000469\ttrain_accuracy\t1.000000\ttest_accuracy\t1.000000\n",
      "iter\t25000\tloss\t0.000094\ttrain_accuracy\t1.000000\ttest_accuracy\t1.000000\n",
      "iter\t26000\tloss\t0.001111\ttrain_accuracy\t1.000000\ttest_accuracy\t1.000000\n",
      "iter\t27000\tloss\t0.009683\ttrain_accuracy\t1.000000\ttest_accuracy\t1.000000\n",
      "iter\t28000\tloss\t0.002317\ttrain_accuracy\t1.000000\ttest_accuracy\t1.000000\n",
      "iter\t29000\tloss\t0.000785\ttrain_accuracy\t1.000000\ttest_accuracy\t1.000000\n"
     ]
    }
   ],
   "source": [
    "model = ConvModel(0.001, 64, 30000)   # 学习率为0.001，每批传入64张图，训练30000次\n",
    "model.train()      # 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function conv2d in module tensorflow.python.layers.convolutional:\n",
      "\n",
      "conv2d(inputs, filters, kernel_size, strides=(1, 1), padding='valid', data_format='channels_last', dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer=None, bias_initializer=<tensorflow.python.ops.init_ops.Zeros object at 0x7fd7281e82b0>, kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, trainable=True, name=None, reuse=None)\n",
      "    Functional interface for the 2D convolution layer.\n",
      "    \n",
      "    This layer creates a convolution kernel that is convolved\n",
      "    (actually cross-correlated) with the layer input to produce a tensor of\n",
      "    outputs. If `use_bias` is True (and a `bias_initializer` is provided),\n",
      "    a bias vector is created and added to the outputs. Finally, if\n",
      "    `activation` is not `None`, it is applied to the outputs as well.\n",
      "    \n",
      "    Arguments:\n",
      "      inputs: Tensor input.\n",
      "      filters: Integer, the dimensionality of the output space (i.e. the number\n",
      "        of filters in the convolution).\n",
      "      kernel_size: An integer or tuple/list of 2 integers, specifying the\n",
      "        height and width of the 2D convolution window.\n",
      "        Can be a single integer to specify the same value for\n",
      "        all spatial dimensions.\n",
      "      strides: An integer or tuple/list of 2 integers,\n",
      "        specifying the strides of the convolution along the height and width.\n",
      "        Can be a single integer to specify the same value for\n",
      "        all spatial dimensions.\n",
      "        Specifying any stride value != 1 is incompatible with specifying\n",
      "        any `dilation_rate` value != 1.\n",
      "      padding: One of `\"valid\"` or `\"same\"` (case-insensitive).\n",
      "      data_format: A string, one of `channels_last` (default) or `channels_first`.\n",
      "        The ordering of the dimensions in the inputs.\n",
      "        `channels_last` corresponds to inputs with shape\n",
      "        `(batch, height, width, channels)` while `channels_first` corresponds to\n",
      "        inputs with shape `(batch, channels, height, width)`.\n",
      "    \n",
      "      dilation_rate: An integer or tuple/list of 2 integers, specifying\n",
      "        the dilation rate to use for dilated convolution.\n",
      "        Can be a single integer to specify the same value for\n",
      "        all spatial dimensions.\n",
      "        Currently, specifying any `dilation_rate` value != 1 is\n",
      "        incompatible with specifying any stride value != 1.\n",
      "      activation: Activation function. Set it to None to maintain a\n",
      "        linear activation.\n",
      "      use_bias: Boolean, whether the layer uses a bias.\n",
      "      kernel_initializer: An initializer for the convolution kernel.\n",
      "      bias_initializer: An initializer for the bias vector. If None, the default\n",
      "        initializer will be used.\n",
      "      kernel_regularizer: Optional regularizer for the convolution kernel.\n",
      "      bias_regularizer: Optional regularizer for the bias vector.\n",
      "      activity_regularizer: Optional regularizer function for the output.\n",
      "      kernel_constraint: Optional projection function to be applied to the\n",
      "          kernel after being updated by an `Optimizer` (e.g. used to implement\n",
      "          norm constraints or value constraints for layer weights). The function\n",
      "          must take as input the unprojected variable and must return the\n",
      "          projected variable (which must have the same shape). Constraints are\n",
      "          not safe to use when doing asynchronous distributed training.\n",
      "      bias_constraint: Optional projection function to be applied to the\n",
      "          bias after being updated by an `Optimizer`.\n",
      "      trainable: Boolean, if `True` also add variables to the graph collection\n",
      "        `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).\n",
      "      name: A string, the name of the layer.\n",
      "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
      "        by the same name.\n",
      "    \n",
      "    Returns:\n",
      "      Output tensor.\n",
      "    \n",
      "    Raises:\n",
      "      ValueError: if eager execution is enabled.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.layers.conv2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
