{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'zeros_4:0' shape=(2, 4) dtype=float32>,\n",
       " <tf.Tensor 'zeros_4:0' shape=(2, 4) dtype=float32>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSTM_CELL_SIZE = 4  # 输出尺寸（维度），与单元格中的隐藏尺寸相同\n",
    "\n",
    "lstm_cell = tf.contrib.rnn.BasicLSTMCell(LSTM_CELL_SIZE, state_is_tuple=True)\n",
    "state = (tf.zeros([2,LSTM_CELL_SIZE]),)*2\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2. 3. 4. 3. 2.]\n",
      " [3. 2. 2. 2. 2. 2.]]\n"
     ]
    }
   ],
   "source": [
    "sample_input = tf.constant([[1,2,3,4,3,2],[3,2,2,2,2,2]],dtype=tf.float32)\n",
    "print (sess.run(sample_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMStateTuple(c=array([[-0.43529567,  0.03958813,  0.8070472 ,  0.52889174],\n",
      "       [ 0.20931017,  0.1332616 , -0.60053825,  0.25247544]],\n",
      "      dtype=float32), h=array([[-0.14358418,  0.00666821,  0.32217664,  0.07481965],\n",
      "       [ 0.14165983,  0.01227455, -0.42908242,  0.08851139]],\n",
      "      dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"LSTM_sample3\"):\n",
    "    output, state_new = lstm_cell(sample_input, state)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print (sess.run(state_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class BasicLSTMCell in module tensorflow.python.ops.rnn_cell_impl:\n",
      "\n",
      "class BasicLSTMCell(_LayerRNNCell)\n",
      " |  Basic LSTM recurrent network cell.\n",
      " |  \n",
      " |  The implementation is based on: http://arxiv.org/abs/1409.2329.\n",
      " |  \n",
      " |  We add forget_bias (default: 1) to the biases of the forget gate in order to\n",
      " |  reduce the scale of forgetting in the beginning of the training.\n",
      " |  \n",
      " |  It does not allow cell clipping, a projection layer, and does not\n",
      " |  use peep-hole connections: it is the basic baseline.\n",
      " |  \n",
      " |  For advanced models, please use the full @{tf.nn.rnn_cell.LSTMCell}\n",
      " |  that follows.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      BasicLSTMCell\n",
      " |      _LayerRNNCell\n",
      " |      RNNCell\n",
      " |      tensorflow.python.layers.base.Layer\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, num_units, forget_bias=1.0, state_is_tuple=True, activation=None, reuse=None, name=None)\n",
      " |      Initialize the basic LSTM cell.\n",
      " |      \n",
      " |      Args:\n",
      " |        num_units: int, The number of units in the LSTM cell.\n",
      " |        forget_bias: float, The bias added to forget gates (see above).\n",
      " |          Must set to `0.0` manually when restoring from CudnnLSTM-trained\n",
      " |          checkpoints.\n",
      " |        state_is_tuple: If True, accepted and returned states are 2-tuples of\n",
      " |          the `c_state` and `m_state`.  If False, they are concatenated\n",
      " |          along the column axis.  The latter behavior will soon be deprecated.\n",
      " |        activation: Activation function of the inner states.  Default: `tanh`.\n",
      " |        reuse: (optional) Python boolean describing whether to reuse variables\n",
      " |          in an existing scope.  If not `True`, and the existing scope already has\n",
      " |          the given variables, an error is raised.\n",
      " |        name: String, the name of the layer. Layers with the same name will\n",
      " |          share weights, but to avoid mistakes we require reuse=True in such\n",
      " |          cases.\n",
      " |      \n",
      " |        When restoring from CudnnLSTM-trained checkpoints, must use\n",
      " |        `CudnnCompatibleLSTMCell` instead.\n",
      " |  \n",
      " |  build(self, inputs_shape)\n",
      " |  \n",
      " |  call(self, inputs, state)\n",
      " |      Long short-term memory cell (LSTM).\n",
      " |      \n",
      " |      Args:\n",
      " |        inputs: `2-D` tensor with shape `[batch_size, input_size]`.\n",
      " |        state: An `LSTMStateTuple` of state tensors, each shaped\n",
      " |          `[batch_size, self.state_size]`, if `state_is_tuple` has been set to\n",
      " |          `True`.  Otherwise, a `Tensor` shaped\n",
      " |          `[batch_size, 2 * self.state_size]`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A pair containing the new hidden state, and the new state (either a\n",
      " |          `LSTMStateTuple` or a concatenated state, depending on\n",
      " |          `state_is_tuple`).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  output_size\n",
      " |  \n",
      " |  state_size\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _LayerRNNCell:\n",
      " |  \n",
      " |  __call__(self, inputs, state, scope=None, *args, **kwargs)\n",
      " |      Run this RNN cell on inputs, starting from the given state.\n",
      " |      \n",
      " |      Args:\n",
      " |        inputs: `2-D` tensor with shape `[batch_size, input_size]`.\n",
      " |        state: if `self.state_size` is an integer, this should be a `2-D Tensor`\n",
      " |          with shape `[batch_size, self.state_size]`.  Otherwise, if\n",
      " |          `self.state_size` is a tuple of integers, this should be a tuple\n",
      " |          with shapes `[batch_size, s] for s in self.state_size`.\n",
      " |        scope: optional cell scope.\n",
      " |        *args: Additional positional arguments.\n",
      " |        **kwargs: Additional keyword arguments.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A pair containing:\n",
      " |      \n",
      " |        - Output: A `2-D` tensor with shape `[batch_size, self.output_size]`.\n",
      " |        - New state: Either a single `2-D` tensor, or a tuple of tensors matching\n",
      " |          the arity and shapes of `state`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from RNNCell:\n",
      " |  \n",
      " |  zero_state(self, batch_size, dtype)\n",
      " |      Return zero-filled state tensor(s).\n",
      " |      \n",
      " |      Args:\n",
      " |        batch_size: int, float, or unit Tensor representing the batch size.\n",
      " |        dtype: the data type to use for the state.\n",
      " |      \n",
      " |      Returns:\n",
      " |        If `state_size` is an int or TensorShape, then the return value is a\n",
      " |        `N-D` tensor of shape `[batch_size, state_size]` filled with zeros.\n",
      " |      \n",
      " |        If `state_size` is a nested list or tuple, then the return value is\n",
      " |        a nested list or tuple (of the same structure) of `2-D` tensors with\n",
      " |        the shapes `[batch_size, s]` for each s in `state_size`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tensorflow.python.layers.base.Layer:\n",
      " |  \n",
      " |  __deepcopy__(self, memo)\n",
      " |  \n",
      " |  add_loss(self, losses, inputs=None)\n",
      " |      Add loss tensor(s), potentially dependent on layer inputs.\n",
      " |      \n",
      " |      Some losses (for instance, activity regularization losses) may be dependent\n",
      " |      on the inputs passed when calling a layer. Hence, when reusing the same\n",
      " |      layer on different inputs `a` and `b`, some entries in `layer.losses` may\n",
      " |      be dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      The `get_losses_for` method allows to retrieve the losses relevant to a\n",
      " |      specific set of inputs.\n",
      " |      \n",
      " |      Note that `add_loss` is not supported when executing eagerly. Instead,\n",
      " |      variable regularizers may be added through `add_variable`. Activity\n",
      " |      regularization is not supported directly (but such losses may be returned\n",
      " |      from `Layer.call()`).\n",
      " |      \n",
      " |      Arguments:\n",
      " |        losses: Loss tensor, or list/tuple of tensors.\n",
      " |        inputs: Optional input tensor(s) that the loss(es) depend on. Must\n",
      " |          match the `inputs` argument passed to the `__call__` method at the time\n",
      " |          the losses are created. If `None` is passed, the losses are assumed\n",
      " |          to be unconditional, and will apply across all dataflows of the layer\n",
      " |          (e.g. weight regularization losses).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  add_update(self, updates, inputs=None)\n",
      " |      Add update op(s), potentially dependent on layer inputs.\n",
      " |      \n",
      " |      Weight updates (for instance, the updates of the moving mean and variance\n",
      " |      in a BatchNormalization layer) may be dependent on the inputs passed\n",
      " |      when calling a layer. Hence, when reusing the same layer on\n",
      " |      different inputs `a` and `b`, some entries in `layer.updates` may be\n",
      " |      dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      The `get_updates_for` method allows to retrieve the updates relevant to a\n",
      " |      specific set of inputs.\n",
      " |      \n",
      " |      This call is ignored in Eager mode.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        updates: Update op, or list/tuple of update ops.\n",
      " |        inputs: Optional input tensor(s) that the update(s) depend on. Must\n",
      " |          match the `inputs` argument passed to the `__call__` method at the time\n",
      " |          the updates are created. If `None` is passed, the updates are assumed\n",
      " |          to be unconditional, and will apply across all dataflows of the layer.\n",
      " |  \n",
      " |  add_variable(self, name, shape, dtype=None, initializer=None, regularizer=None, trainable=True, constraint=None, partitioner=None)\n",
      " |      Adds a new variable to the layer, or gets an existing one; returns it.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        name: variable name.\n",
      " |        shape: variable shape.\n",
      " |        dtype: The type of the variable. Defaults to `self.dtype` or `float32`.\n",
      " |        initializer: initializer instance (callable).\n",
      " |        regularizer: regularizer instance (callable).\n",
      " |        trainable: whether the variable should be part of the layer's\n",
      " |          \"trainable_variables\" (e.g. variables, biases)\n",
      " |          or \"non_trainable_variables\" (e.g. BatchNorm mean, stddev).\n",
      " |          Note, if the current variable scope is marked as non-trainable\n",
      " |          then this parameter is ignored and any added variables are also\n",
      " |          marked as non-trainable.\n",
      " |        constraint: constraint instance (callable).\n",
      " |        partitioner: (optional) partitioner instance (callable).  If\n",
      " |          provided, when the requested variable is created it will be split\n",
      " |          into multiple partitions according to `partitioner`.  In this case,\n",
      " |          an instance of `PartitionedVariable` is returned.  Available\n",
      " |          partitioners include `tf.fixed_size_partitioner` and\n",
      " |          `tf.variable_axis_size_partitioner`.  For more details, see the\n",
      " |          documentation of `tf.get_variable` and the  \"Variable Partitioners\n",
      " |          and Sharding\" section of the API guide.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The created variable.  Usually either a `Variable` or `ResourceVariable`\n",
      " |        instance.  If `partitioner` is not `None`, a `PartitionedVariable`\n",
      " |        instance is returned.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called with partioned variable regularization and\n",
      " |          eager execution is enabled.\n",
      " |  \n",
      " |  apply(self, inputs, *args, **kwargs)\n",
      " |      Apply the layer on a input.\n",
      " |      \n",
      " |      This simply wraps `self.__call__`.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor(s).\n",
      " |        *args: additional positional arguments to be passed to `self.call`.\n",
      " |        **kwargs: additional keyword arguments to be passed to `self.call`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor(s).\n",
      " |  \n",
      " |  compute_output_shape(self, input_shape)\n",
      " |      Computes the output shape of the layer given the input shape.\n",
      " |      \n",
      " |      Args:\n",
      " |        input_shape: A (possibly nested tuple of) `TensorShape`.  It need not\n",
      " |          be fully defined (e.g. the batch size may be unknown).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A (possibly nested tuple of) `TensorShape`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: if `input_shape` is not a (possibly nested tuple of)\n",
      " |          `TensorShape`.\n",
      " |        ValueError: if `input_shape` is incomplete or is incompatible with the\n",
      " |          the layer.\n",
      " |  \n",
      " |  count_params(self)\n",
      " |      Count the total number of scalars composing the weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An integer count.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: if the layer isn't yet built\n",
      " |            (in which case its weights aren't yet defined).\n",
      " |  \n",
      " |  get_input_at(self, node_index)\n",
      " |      Retrieves the input tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_input_shape_at(self, node_index)\n",
      " |      Retrieves the input shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_losses_for(self, inputs)\n",
      " |      Retrieves losses relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |          Must match the `inputs` argument passed to the `__call__`\n",
      " |          method at the time the losses were created.\n",
      " |          If you pass `inputs=None`, unconditional losses are returned,\n",
      " |          such as weight regularization losses.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of loss tensors of the layer that depend on `inputs`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_output_at(self, node_index)\n",
      " |      Retrieves the output tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_output_shape_at(self, node_index)\n",
      " |      Retrieves the output shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_updates_for(self, inputs)\n",
      " |      Retrieves updates relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |          Must match the `inputs` argument passed to the `__call__` method\n",
      " |          at the time the updates were created.\n",
      " |          If you pass `inputs=None`, unconditional updates are returned.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of update ops of the layer that depend on `inputs`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.layers.base.Layer:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  activity_regularizer\n",
      " |      Optional regularizer function for the output of this layer.\n",
      " |  \n",
      " |  dtype\n",
      " |  \n",
      " |  graph\n",
      " |  \n",
      " |  inbound_nodes\n",
      " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
      " |  \n",
      " |  input\n",
      " |      Retrieves the input tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input tensor or list of input tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |        AttributeError: If no inbound nodes are found.\n",
      " |  \n",
      " |  input_shape\n",
      " |      Retrieves the input shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer, or if all inputs\n",
      " |      have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per input tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined input_shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  losses\n",
      " |      Losses which are associated with this `Layer`.\n",
      " |      \n",
      " |      Note that when executing eagerly, getting this property evaluates\n",
      " |      regularizers. When using graph execution, variable regularization ops have\n",
      " |      already been created and are simply returned here.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of tensors.\n",
      " |  \n",
      " |  name\n",
      " |  \n",
      " |  non_trainable_variables\n",
      " |  \n",
      " |  non_trainable_weights\n",
      " |  \n",
      " |  outbound_nodes\n",
      " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
      " |  \n",
      " |  output\n",
      " |      Retrieves the output tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one output,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor or list of output tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        AttributeError: if the layer is connected to more than one incoming\n",
      " |          layers.\n",
      " |        RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  output_shape\n",
      " |      Retrieves the output shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has one output,\n",
      " |      or if all outputs have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per output tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined output shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  scope_name\n",
      " |  \n",
      " |  trainable_variables\n",
      " |  \n",
      " |  trainable_weights\n",
      " |  \n",
      " |  updates\n",
      " |  \n",
      " |  variables\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  weights\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.contrib.rnn.BasicLSTMCell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 堆叠LSTM\n",
    "sess = tf.Session()\n",
    "LSTM_CELL_SIZE = 4  # 输出尺寸（维度），与单元格中的隐藏尺寸相同 \n",
    "input_dim = 6        \n",
    "num_layers = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "cells = []\n",
    "for _ in range(num_layers):\n",
    "    cell = tf.contrib.rnn.LSTMCell(LSTM_CELL_SIZE,)\n",
    "    cells.append(cell)\n",
    "stacked_lstm = tf.contrib.rnn.MultiRNNCell(cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.placeholder(tf.float32, [None, None, input_dim])\n",
    "output, state = tf.nn.dynamic_rnn(cell, data, dtype=tf.float32)\n",
    "\n",
    "sample_input = [[[1,2,3,4,3,2], [1,2,1,1,1,2],[1,2,2,2,2,2]],\n",
    "                [[1,2,3,4,3,2], [1,2,1,1,1,2],[1,2,2,2,2,2]],\n",
    "                [[1,2,3,4,3,2],[3,2,2,1,1,2],[0,0,0,0,3,2]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.13624077,  0.13096371,  0.12722576,  0.00196907],\n",
       "        [ 0.04306107,  0.22711045,  0.22253367, -0.06120773],\n",
       "        [ 0.05338787,  0.2632047 ,  0.23139995, -0.02738318]],\n",
       "\n",
       "       [[ 0.13624077,  0.13096371,  0.12722576,  0.00196907],\n",
       "        [ 0.04306107,  0.22711045,  0.22253367, -0.06120773],\n",
       "        [ 0.05338787,  0.2632047 ,  0.23139995, -0.02738318]],\n",
       "\n",
       "       [[ 0.13624077,  0.13096371,  0.12722576,  0.00196907],\n",
       "        [ 0.1279214 ,  0.19765474,  0.2838099 ,  0.00835462],\n",
       "        [-0.08375914,  0.2291002 ,  0.35174415, -0.09680846]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(output, feed_dict={data: sample_input})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
