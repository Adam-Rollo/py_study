{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>帮我</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>查下</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>明天</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>北京</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>天气</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id word\n",
       "0   1   帮我\n",
       "1   1   查下\n",
       "2   1   明天\n",
       "3   1   北京\n",
       "4   1   天气"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 首先导入所需要的库\n",
    "\n",
    "import nltk\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "\n",
    "df = pd.read_csv('corpus.csv', encoding='gbk')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8] ['帮我', '查下', '明天', '北京', '天气', '怎么样', '帮我', '查下', '今天', '北京', '天气', '好不好', '帮我', '查询', '去', '北京', '的', '火车', '帮我', '查看', '到', '上海', '的', '火车', '帮我', '查看', '特朗普', '的', '新闻', '帮我', '看看', '有没有', '北京', '的', '新闻', '帮我', '搜索', '上海', '有', '什么', '好玩的', '帮我', '找找', '上海', '东方明珠', '在哪']\n"
     ]
    }
   ],
   "source": [
    "# 将数据转化成列表\n",
    "\n",
    "id_list = list(df.id)\n",
    "\n",
    "word_list = list(df.word)\n",
    "\n",
    "print(id_list, word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['帮我', '查下', '明天', '北京', '天气', '怎么样']\n",
      "['帮我', '查下', '今天', '北京', '天气', '好不好']\n",
      "['帮我', '查询', '去', '北京', '的', '火车']\n",
      "['帮我', '查看', '到', '上海', '的', '火车']\n",
      "['帮我', '查看', '特朗普', '的', '新闻']\n",
      "['帮我', '看看', '有没有', '北京', '的', '新闻']\n",
      "['帮我', '搜索', '上海', '有', '什么', '好玩的']\n",
      "['帮我', '找找', '上海', '东方明珠', '在哪']\n",
      "['帮我 查下 明天 北京 天气 怎么样', '帮我 查下 今天 北京 天气 好不好', '帮我 查询 去 北京 的 火车', '帮我 查看 到 上海 的 火车', '帮我 查看 特朗普 的 新闻', '帮我 看看 有没有 北京 的 新闻', '帮我 搜索 上海 有 什么 好玩的', '帮我 找找 上海 东方明珠 在哪']\n"
     ]
    }
   ],
   "source": [
    "cps = []\n",
    "\n",
    "for i in df.groupby(['id']):\n",
    "\n",
    "    seg_list = list(i[1].word)\n",
    "\n",
    "    print(seg_list)\n",
    "\n",
    "    seg = ' '.join(seg_list)\n",
    "\n",
    "    cps.append(seg)\n",
    "\n",
    "print(cps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'查下': 1, '怎么样': 1, '帮我': 1, '明天': 1, '天气': 1, '北京': 1}\n",
      "{'查下': 1, '帮我': 1, '好不好': 1, '天气': 1, '北京': 1, '今天': 1}\n",
      "{'查询': 1, '去': 1, '帮我': 1, '的': 1, '火车': 1, '北京': 1}\n",
      "{'上海': 1, '帮我': 1, '的': 1, '到': 1, '火车': 1, '查看': 1}\n",
      "{'特朗普': 1, '帮我': 1, '查看': 1, '的': 1, '新闻': 1}\n",
      "{'帮我': 1, '的': 1, '有没有': 1, '看看': 1, '北京': 1, '新闻': 1}\n",
      "{'上海': 1, '搜索': 1, '有': 1, '帮我': 1, '什么': 1, '好玩的': 1}\n",
      "{'上海': 1, '东方明珠': 1, '帮我': 1, '找找': 1, '在哪': 1}\n"
     ]
    }
   ],
   "source": [
    "# 计算词频\n",
    "\n",
    "import nltk\n",
    "\n",
    "for i in cps:\n",
    "\n",
    "    ii = i.split(' ')\n",
    "\n",
    "    cfd = dict(nltk.FreqDist(ii))\n",
    "\n",
    "    print(cfd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of words:\n",
      "['上海', '东方明珠', '什么', '今天', '到', '北京', '去', '在哪', '天气', '好不好', '好玩的', '帮我', '怎么样', '找找', '搜索', '新闻', '明天', '有', '有没有', '查下', '查看', '查询', '火车', '特朗普', '的', '看看']\n",
      "len of bag_of_words: 26\n"
     ]
    }
   ],
   "source": [
    "# 运用sklearn的BOW方法\n",
    "\n",
    "\n",
    "\n",
    "# step 1 声明一个向量化工具vectorizer\n",
    "\n",
    "vectoerizer = CountVectorizer(min_df=1, max_df=1.0, token_pattern='\\\\b\\\\w+\\\\b')\n",
    "\n",
    "# step 2 根据语料集统计词袋（fit）\n",
    "\n",
    "vectoerizer.fit(cps)\n",
    "\n",
    "# step 3 打印语料集的词袋信息\n",
    "\n",
    "bag_of_words = vectoerizer.get_feature_names()\n",
    "\n",
    "print(\"Bag of words:\")\n",
    "\n",
    "print(bag_of_words)\n",
    "\n",
    "print(\"len of bag_of_words:\", len(bag_of_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized corpus:\n",
      "[[0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0]\n",
      " [1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 1 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 1 1]\n",
      " [1 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "index of `的` is : 24\n"
     ]
    }
   ],
   "source": [
    "# step 4 将语料集转化为词袋向量（transform）\n",
    "\n",
    "X = vectoerizer.transform(cps)\n",
    "\n",
    "# 查看数值特征\n",
    "\n",
    "print(\"Vectorized corpus:\")\n",
    "\n",
    "print(X.toarray())\n",
    "\n",
    "# step 5 还可以查看每个词在词袋中的索引\n",
    "# 每一行的1代表索引数字的字是包含的，0代表不包含的\n",
    "print(\"index of `的` is : {}\".format(vectoerizer.vocabulary_.get('的')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1 声明一个TF-IDF转化器（TfidfTransformer）\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "# step 2 根据语料集的词袋向量计算TF-IDF（fit）\n",
    "\n",
    "tfidf_transformer.fit(X.toarray())\n",
    "\n",
    "# step 3 打印TF-IDF信息：比如结合词袋信息，可以查看每个词的TF-IDF值\n",
    "\n",
    "for idx, word in enumerate(vectoerizer.get_feature_names()):\n",
    "\n",
    "  print(\"{}\\t{}\".format(word, tfidf_transformer.idf_[idx]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
